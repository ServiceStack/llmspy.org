---
title: Introduction
description: Introducing llms.py - Lightweight OpenAI compatible CLI and server gateway
---

# Introducing llms.py üöÄ

We're excited to announce **[llms.py](https://github.com/ServiceStack/llms)** - a super lightweight CLI tool and OpenAI-compatible server that acts as a **configurable gateway** over multiple configurable Large Language Model (LLM) providers.

## Why llms.py?

As part of our work in developing a new OSS AI Generation platform, we needed a lightweight LLM gateway for usage within [ComfyUI](https://www.comfy.org). Unfortunately, the popular Python option **litellm** requires [60+ dependencies](https://github.com/BerriAI/litellm/blob/main/requirements.txt), which is a deal breaker in an open Python plugin ecosystem like ComfyUI where every dependency has a chance to break a Python environment.

## üéØ OpenRouter but Local

**llms.py** is designed as a **unified gateway** that seamlessly connects you to multiple LLM providers through a single, consistent interface. Whether using cloud APIs or local models, `llms` provides intelligent routing and automatic failover to ensure your AI workflows connect to your chosen providers in your preferred priority - whether optimizing for cost, performance, or availability.

### ‚ö° Ultra-Lightweight Architecture

- **Single File**: Just one [llms.py](https://github.com/ServiceStack/llms/blob/main/llms/main.py) file (easily customizable)
- **Single Dependency**: Only requires `aiohttp`
- **Zero Dependencies for ComfyUI**: Ideal for use in ComfyUI Custom Nodes
- **No Setup**: Just download and use, configure preferred LLMs in [llms.json](https://github.com/ServiceStack/llms/blob/main/llms/llms.json)

## üåê Configurable Multi-Provider Gateway

Acts as an intelligent gateway that can route requests for 160+ models across:

### Cloud Providers with Free Tiers

- OpenRouter
- Groq
- Codestral
- Google

### Premium Cloud Providers

- OpenAI
- Anthropic
- Google
- Grok
- Qwen
- Mistral

### Local Providers

- Ollama
  - Restrict access to custom models
  - Or auto-discovery of installed models

### Custom Providers

Use JSON config to add any OpenAI-compatible API endpoints and models

## üîÑ Intelligent Request Routing

- **Automatic Failover**: If one provider fails, automatically retry with the next available provider
- **Cost Optimization**: Define free/cheap/local providers first to minimize costs
- **Model Mapping**: Use unified model names that map to different provider-specific names

## üöÄ Key Features

### Multi-Modal Support
- **Text Generation**: Chat completions with any supported model
- **Vision Models**: Process images through vision-capable models (GPT-4V, Gemini Vision, etc.)
- **Audio Processing**: Handle audio inputs through audio-capable models
- **Document Processing**: Analyze PDFs and documents with capable models

### Flexible Deployment Options
- **CLI Tool**: Interactive command-line interface for quick queries
- **HTTP Server**: OpenAI-compatible server at `http://localhost:{PORT}/v1/chat/completions`
- **Python Module**: Import and use programmatically in your applications
- **ComfyUI Node**: Embed directly in ComfyUI workflows

### Simple and Customizable
- **Environment Variables**: Secure API key management
- **Provider Management**: Easy enable/disable of providers
- **Custom Models**: Define your own model aliases and mappings
- **Unified Configuration**: Single [llms.json](https://github.com/ServiceStack/llms/blob/main/llms/llms.json) to configure all providers and models

## üåü Why Choose llms.py?

1. **Simplicity**: One file, one dependency, infinite possibilities
2. **Flexibility**: Works with any OpenAI-compatible client or framework
3. **Reliability**: Automatic failover ensures your workflows never break
4. **Economy**: Intelligent routing minimizes API costs
5. **Privacy**: Mix local and cloud models based on your data sensitivity
6. **Future-Proof**: Easily add new providers as they emerge

**llms.py** transforms the complexity of managing multiple LLM providers into a simple, unified experience. Whether you're researching capabilities of new models, building the next breakthrough AI application, or just want reliable access to the best models available, llms.py has you covered.

Get started today and avoid expensive cloud lock-ins with the freedom of provider-agnostic AI development! üéâ

## Links

- üìö [GitHub Repository](https://github.com/ServiceStack/llms)
- üì¶ [PyPI Package](https://pypi.org/project/llms-py/)
- üîß [Source Code](https://github.com/ServiceStack/llms)
