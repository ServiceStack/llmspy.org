---
title: v3 Release Notes
description: Major release focused on extensibility, expanded provider support, and enhanced user experience.
---

## üöÄ What's New at a Glance

| Feature | Description |
|---------|-------------|
| **530+ Models** | Access over 530 models from 23 providers via [models.dev](https://models.dev) integration |
| **Extensions System** | Add features, providers, and customize the UI with a flexible plugin architecture |
| **Tool Support** | First-class Python function calling for LLM interactions with your local environment |
| **New Model Selector** | Redesigned full-featured dialog with search, filtering, sorting, and favorites |
| **Image Generation** | Built-in support for Google, OpenAI, OpenRouter, Chutes, and Nvidia |
| **Audio Generation** | TTS support for Gemini 2.5 Flash/Pro Preview models |
| **Media Gallery** | Beautiful UI to browse generated portrait/landscape images and audio generations |
| **Run Code UI** | Execute Python, JavaScript, TypeScript and C# code scripts in a CodeMirror editor |
| **Calculator UI** | Beautiful UX Friendly UI to evaluate python math expressions |
| **KaTeX Support** | Support for beautiful rendering of LaTeX math expressions |
| **SQLite Storage** | Migrated from IndexedDB to server SQLite for robust persistence and concurrent usage |
| **Asset Caching** | Persistent image/file file caching with metadata |

---

## Table of Contents

- [Rewritten for Extensibility](#rewritten-for-extensibility)
- [New Provider Configuration Model](#new-provider-configuration-model)
- [New Model Selector UI](#new-model-selector-ui)
- [Extensions System](#extensions-system)
- [Tool Support](#tool-support)
- [Available Tools](#available-tools)
- [Image Generation Support](#image-generation-support)
- [Audio Generation Support](#audio-generation-support)
- [Image Cache & Optimization](#image-cache--optimization)
- [Enhancements & Fixes](#enhancements--fixes)
- [v3 Configuration Migration](#v3-configuration-migration)

---

## Rewritten for Extensibility

llms.py has been rewritten from the ground-up with extensibility a **core concept** where all [major UI and Server features](https://github.com/ServiceStack/llms/tree/main/llms/extensions) now layer on their encapsulated functionality by using the public Client & Server Extensibility APIs.

Extensions are just folders that can add both Server and UI features using the public client and server extensibility APIs. Built-in features are just extensions in the repo's [llms/extensions](https://github.com/ServiceStack/llms/tree/main/llms/extensions) folder which can be overridden by adding them to your local `~/.llms/extensions` folder.

llms includes support for installing and uninstalling extensions from any GitHub repository. For better discoverability, non built-in extensions are maintained in the [github.com/llmspy](https://github.com/orgs/llmspy/repositories) organization repositories which anyone else is welcome to contribute their repos to for increased discoverability.

UI components are now registered and referenced as Global Vue components, which can be easily replaced by registering Vue components with the same name as done in the [xmas](https://github.com/llmspy/xmas/blob/main/ui/index.mjs) extension demo.

This approach allows [main.py](https://github.com/ServiceStack/llms/blob/main/llms/main.py) to retain a **lean functional core in a single file** whilst still being fully extensible and lays the foundation for **rapid development of new features** - both from the core team and external 3rd party extensions - enabling the community to extend llms.py in new unanticipated ways.

## Server SQLite and Cached File Storage persistence

Another major change is the migration from client-side IndexedDB storage to a robust server-side SQLite solution. This architectural shift ensures better data consistency, improved performance, and enables parallel executions and multi-device access to your chat history.

To keep the database efficient and portable, binary assets (images, audio, etc.) are not stored directly in the SQLite database, Instead all generated assets are stored in the local file system cache at `~/.llms/cache` and only **relative URLs** referencing these assets are stored in the database.

#### Concurrency Model
To ensure data integrity and high performance without complex locking mechanisms, the system utilizes a **single background thread** for managing all write operations to the database. This design improves concurrency handling and eliminates database locking issues during high-load scenarios.

#### Multi-Tenancy & Security
When authentication is enabled, data isolation is automatically enforced. All core tables, including `threads` and `requests`, are scoped to the authenticated user, ensuring that users can only access their own data.

---

## New Provider Configuration Model

The most significant change is the migration to utilize the same [models.dev](https://models.dev) open provider and model catalogue as used and maintained by [OpenCode](https://opencode.ai).

**llms.json** provider configuration is now a **superset** of `models.dev/api.json` where its definitions are merged, allowing you to enable providers using just `"enabled": true` to inherit provider configurations from **models.dev**

### üåê Expanded Provider Support

The switch to [models.dev](https://models.dev) greatly expands the model selection to over **530 models** from **23 different providers**, including new support for:

| Provider       | Models   | Provider        | Models   |
|----------------|----------|-----------------|----------|
| Alibaba        | 39       | Hugging Face    | 14       |
| Chutes         | 56       | Zai Coding Plan | 6        |
| DeepSeek       | 2        | MiniMax         | 1        |
| Fireworks AI   | 12       | Moonshot AI     | 5        |
| GitHub Copilot | 27       | Nvidia          | 24       |
| GitHub Models  | 55       | Zai             | 6        |
| LMStudio       | local    | Ollama          | local    |

Non OpenAI Compatible LLM and Image generation providers are maintained in the [providers](https://github.com/ServiceStack/llms/tree/main/llms/extensions/providers) extension, registered using the `ctx.add_provider()` API.

<Tip>üí° [Raise an issue](https://github.com/ServiceStack/llms/issues) to add support for any missing providers from [models.dev](https://models.dev) you would like to use.</Tip>

### üîÑ Automatic Provider Updates

This actively maintained list of available providers and models are automatically updated into your `providers.json` daily that can also be manually updated with:

```bash
llms --update-providers
```

As an optimization only the providers that are referenced in your `llms.json` are saved. Any additional providers you want to use that are not included in models.dev can be added to your `~/.llms/providers-extra.json`, which get merged into your `providers.json` on every update.

This keeps your local configuration file lightweight by only including the providers that are available for use.

### Configuration Examples

Enable providers by ID ‚Äî all configuration is automatically inherited:

```json
{
  "openai": { "enabled": true },
  "xai": { "enabled": true }
}
```

See [Configuration](/docs/configuration) docs for more info.

---

## New Model Selector UI

With over 530 models from 23 providers now available, discovering and selecting the right model required a complete overhaul. 
The Model Selector has been completely redesigned as a full-featured dialog offering:

- **üîç Smart Search & Discovery** - Instantly search across model names, IDs, and providers
- **üéØ Advanced Filtering** - Filter by name, providers & input and output modalities
- **üìä Flexible Sorting** - Sort by Knowledge Cutoff, Release Date, Last Updated & Context
- **‚≠ê Favorites System** - Star model card to add/remove to favorites quick list
- **üíé Rich Model Cards** - In depth model overview at a glance

<a href="/docs/features/model-selector">
<img src="/img/model-selector.webp" class="mx-auto my-4 max-w-2xl rounded-lg hover:shadow border border-gray-200 dark:border-gray-700"/></a>

Where providers can be quickly enabled or disabled to customize which models are available:

<a href="/docs/features/model-selector">
<img src="/img/model-selector-providers.webp" class="mx-auto my-4 max-w-2xl rounded-lg hover:shadow border border-gray-200 dark:border-gray-700"/></a>

See [Model Selector](/docs/features/model-selector) docs for more info.

---

## Extensions System

To keep the core lightweight while enabling limitless enhancements, we've implemented a flexible **Extensions system** inspired by ComfyUI Custom Nodes. This allows adding new features, pages and toolbar icons, register new provider implementations, extend, replace, and customize the UI with your own custom features.

### Installation
Extensions can be installed from GitHub or by creating a local folder:
- **Local**: Simply create a folder in `~/.llms/extensions/my_extension`
- **GitHub**: Clone extensions into `~/.llms/extensions`, e.g:

```
git clone https://github.com/user/repo ~/.llms/extensions/my_extension
```

### Managing Extensions

**List available extensions:**
```bash
llms --add
```

Output:

```
Available extensions:
  xmas         Example of utilizing the Extensions APIs to give llms.py some Christmas spirit
  duckduckgo   Add web search tool capabilities using Duck Duck Go

Usage:
  llms --add <extension>
  llms --add <github-user>/<repo>
```

**Install an extension:**
```bash
llms --add duckduckgo
```

**Install a 3rd-party extension:**
```bash
llms --add my_user/my_extension
```

<Info>Clones the GitHub repo into `~/.llms/extensions/my_extension` and installs any `requirements.txt` dependencies.</Info>

**List installed extensions:**
```bash
llms --remove
```

**Remove an extension:**
```bash
llms --remove system_prompts
```

See [Extensions](/docs/extensions) docs for more details.

### How it Works (Server)
Extensions are Python modules that plug into the server lifecycle using special hooks defined in their `__init__.py`:

| Hook | Purpose |
|------|---------|
| `__parser__(parser)` | Add custom CLI arguments |
| `__install__(ctx)` | Enhance the server instance (routes, providers, filters, etc.) |
| `__run__(ctx)` | Execute custom logic when running in CLI mode |

The `ctx` parameter provides access to the `ExtensionContext`.

See [Server Extensions](/docs/extensions/server) docs for more details.

### How it Works (UI)

Extensions can also include frontend components:

1. **Placement**: Add a `ui` folder within your extension directory
2. **Access**: Files in this folder are automatically served at `/ext/<extension_name>/*`
3. **Integration**: Create a `ui/index.mjs` file. This is the entry point and must export an `install` function:

```javascript
const MyComponent = {
    template: `...`
}

// ui/index.mjs
export default {
    install(ctx) {
        // Register or replace components, add routes, etc.
        ctx.components({ MyComponent })
    }
}
```

See [UI Extensions](/docs/extensions/ui) docs for more details.

### Example: xmas extension
The [xmas](https://github.com/llmspy/xmas) extension demonstrates these capabilities where it utilizes the Extensions APIs to give llms.py a splash of Christmas spirit. It uses `__install__` to register an API endpoint and a UI extension for its UI features.

### Replacing Core Components

All UI features of xmas is implemented in its [ui/index.mjs](https://github.com/llmspy/xmas/blob/main/ui/index.mjs)
which overrides default `Brand` and `Welcome` components by registering components with the same name, e.g:

```js
const Brand = {
    template: `
    <div class="flex-shrink-0 p-2 border-b border-gray-200 dark:border-gray-700 select-none">
        <button type="button" @click="$router.push('/')" class="...">
            üéÑ {{ $state.title }} üéÑ
        </button>
    </div>
    `,
}
const Welcome = {
    template: `<!-- Custom Welcome Screen -->`,
    setup() { /* ... */ }
}

export default {
    install(ctx) {
        ctx.components({
            // Replaces built-in UI Components
            Brand,
            Welcome,
            // Registers other custom components used in this UI Extension
            XmasPage,
            XmasTopPanel,
        })
    }
}
```

To change both the home page and brand on the top-left to give every page title a festive touch:

<ScreenshotsGallery className="mb-8" gridClass="grid grid-cols-1 md:grid-cols-2 gap-4" images={{
    'xmas home': '/img/llms-xmas-home.webp',
}} />

It also demonstrates adding a new icon on the left sidebar to open its custom Xmas page component and a top-panel component to display its "Ask Santa" portal:

<ScreenshotsGallery className="mb-8" gridClass="grid grid-cols-1 md:grid-cols-2 gap-4" images={{
    'xmas page': '/img/llms-xmas-page.webp',
    'Ask Santa panel': '/img/llms-xmas-top.webp',
}} />

The Xmas page calls a custom API endpoint registered in its `__install__` hook to return a custom festive greeting, whilst the top-panel modifies chat requests while its Top Panel is open to add a Santa system prompt which is enough to implement its "Ask Santa" feature.

Smart generation models like Nano Banana's **gemini-2.5-flash-image** perform exceptionally well here as they're able to answer your kids questions with rich, detailed responses and image outputs.

---

## Tool Support

This release also includes **first-class support for Python function calling (Tools)**, allowing LLMs to interact with your local environment and custom functionality. 

Tools can be defined using standard Python functions where its tool definition can be implicitly defined from its function's signature, type hints, and docstrings:

```python
def get_current_time(tz_name: Optional[str] = None) -> str:
    """
    Get current time in ISO-8601 format.

    Args:
        tz_name: Optional timezone name (e.g. 'America/New_York'). Defaults to UTC.
    """
    if tz_name:
        try:
            tz = ZoneInfo(tz_name)
        except Exception:
            return f"Error: Invalid timezone '{tz_name}'"
    else:
        tz = timezone.utc

    return datetime.now(tz).isoformat()
```

### Register tools for function calling

#### Implicit Tool Definition

Tools can be registered within an extension's `install` hook using `ctx.register_tool`:

```python
def install(ctx):
    # Automatic definition from function signature
    ctx.register_tool(get_current_time)
```

#### Explicit Tool Definition

When more fine-grain configuration is needed you can use an explicit tool definition instead, e.g:

```python
ctx.register_tool(
    get_current_time,
    {
        "type": "function",
        "function": {
            "name": "get_current_time",
            "description": "Get current time in ISO-8601 format.",
            "parameters": {
                "type": "object",
                "properties": {
                    "tz_name": {
                        "type": "string",
                        "description": "timezone name (e.g. 'America/New_York')",
                        "default": "UTC"
                    }
                },
                "required": []
            }
        }
    })
```

### UI Management

- **Dedicated Tools Page**: View all registered tools and their definitions at `/tools` or via the sidebar link
- **One-Click Enable/Disable**: Use the Tool Selector in the top-right to control which tools to use per request
- **Granular Control**: Select "All", "None", or specific tools for each chat session

<ScreenshotsGallery className="mb-8" gridClass="grid grid-cols-1 md:grid-cols-2 gap-4" images={{
    'Select tool': '/img/llms-tools-top.webp',
    'Tools Page': '/img/llms-tools-page.webp',
}} />

## Core Tools

The built-in [core_tools](https://github.com/ServiceStack/llms/blob/main/llms/extensions/core_tools/__init__.py) extension provides essential functionality for LLMs to interact with their environment, perform calculations, and manage persistent data.

### Memory Tools

Functions for persistent key-value storage.

* `memory_read` - Read a value from persistent memory.
* `memory_write` - Write a value to persistent memory.

### File System Tools

All file system operations are restricted to the current working directory for safety.

* `read_file` - Read a text file from disk.
* `write_file` - Write text to a file (overwrites existing content).
* `list_directory` - List directory contents including file names, sizes, and modification times.
* `glob_paths` - Find files and directories matching a glob pattern.

<a href="/docs/extensions/tools">
<img src="/img/tool-files.webp" class="mx-auto my-4 max-w-2xl rounded-lg hover:shadow border border-gray-200 dark:border-gray-700"/></a>

### Utilities

* `get_current_time` - Get the current time in ISO-8601 format.

<a href="/docs/extensions/tools">
<img src="/img/llms-tools-get_current_time.webp" class="my-4 rounded-lg hover:shadow border border-gray-200 dark:border-gray-700"/></a>

### Math & Logic

* `calc` - Evaluate a mathematical expression. Supports arithmetic, comparison, boolean operators, and common math functions.

<a href="/docs/extensions/tools">
<img src="/img/llm-tool-call.webp" class="mx-auto my-4 max-w-2xl rounded-lg hover:shadow border border-gray-200 dark:border-gray-700"/></a>

### Code Execution Tools

LLMS includes a suite of tools for executing code in various languages within a sandboxed environment. These tools are designed to allow the agent to run scripts, perform calculations, and verify logic safely.

#### Supported Languages

* `run_python(code)` - Executes Python code.
* `run_javascript(code)` - Executes JavaScript code (uses `bun` or `node`).

<ScreenshotsGallery className="mb-8" gridClass="grid grid-cols-1 md:grid-cols-2 gap-4" images={{
    'Run Python': '/img/tool-python.webp',
    'Run JavaScript': '/img/tool-javascript.webp',
}} />

* `run_typescript(code)` - Executes TypeScript code (uses `bun` or `node`).
* `run_csharp(code)` - Executes C# code (uses `dotnet run` with .NET 10+ single-file support).

<ScreenshotsGallery className="mb-8" gridClass="grid grid-cols-1 md:grid-cols-2 gap-4" images={{
    'Run TypeScript': '/img/tool-typescript.webp',
    'Run C#': '/img/tool-csharp.webp',
}} />

## Calculator UIs

As some of the core tools are particularly useful on their own, dedicated UIs has been added for the `calc` tool with support for evaluating mathematical python expressions which supports arithmetic, comparison, boolean operators, and `math.*` functions and constants and python list comprehensions.

<a href="/docs/extensions/tools">
<img src="/img/run-calc.webp" class="mx-auto my-4 max-w-2xl rounded-lg hover:shadow border border-gray-200 dark:border-gray-700"/></a>

Whilst the `run_python` tools provides a scratch pad for running stand-alone Python, JavaScript, TypeScript, and C# code.

## Run Python, JavaScript, TypeScript & C# programs

<ScreenshotsGallery className="mb-8" gridClass="grid grid-cols-1 md:grid-cols-2 gap-4" images={{
    'Run Python': '/img/run-python.webp',
    'Run JavaScript': '/img/run-javascript.webp',
    'Run TypeScript': '/img/run-typescript.webp',
    'Run C#': '/img/run-csharp.webp',
}} />


---

## Available Tools

All available tools are maintained in the GitHub [llmspy organization](https://github.com/orgs/llmspy/repositories):

| Tool | Description |
|------|-------------|
| `duckduckgo` | Add web search capabilities using DuckDuckGo |
| `xmas` | Example of utilizing the Extensions APIs to give llms.py some Christmas spirit |

**Install a tool:**
```bash
llms --add duckduckgo
```

Installing an extension clones it into your `~/.llms/extensions` folder and installs any Python `requirements.txt` dependencies. You can remove an extension by deleting the folder from `~/.llms/extensions` or by using:

**List installed extensions:**

```bash
llms --remove
```

**Remove installed extensions:**

```bash
llms --remove duckduckgo
```

**Install 3rd-party extensions:**
```bash
llms --add <user>/<repo>
```

**Manual installation:**
```bash
git clone https://github.com/<user>/<repo> ~/.llms/extensions/<repo>
```

<Tip>ü§ù Feel free to submit pull requests to add new extensions to the [llmspy organization](https://github.com/orgs/llmspy/repositories) to make your extension easily discoverable to everyone.</Tip>

---

## Image Generation Support

v3 includes built-in support for image generation models on:

| Provider   | Status |
|------------|--------|
| Google     | ‚úÖ Supported |
| OpenAI     | ‚úÖ Supported |
| OpenRouter | ‚úÖ Supported |
| Chutes     | ‚úÖ Supported |
| Nvidia     | ‚úÖ Supported |

<img src="/img/generate-image.webp" class="my-4 rounded-lg hover:shadow border border-gray-200 dark:border-gray-700"/>

<Info>‚ö†Ô∏è Since there is no standard way to generate images, this required a custom implementation for each provider.</Info>

### Command-Line Usage

Generate images using the `--out image` modifier:

```bash
llms --out image "cat in a hat"
```

**Output:**

```
Here's a cat in a hat for you!

Saved files:
/home/mythz/.llms/cache/ba/ba951f1054cfbf7b9d0bb8d5bc1d9fe1cbc15de0ee63c881b9256d691db4cd2e.png
http://localhost:8000//~cache/ba/ba951f1054cfbf7b9d0bb8d5bc1d9fe1cbc15de0ee63c881b9256d691db4cd2e.png
```

This uses the `out:image` chat template in `llms.json` by default.

### Specify a Model

Use any model that supports image generation by specifying its **ID** or **name**:

```bash
llms -m "gemini-2.5-flash-image" --out image "cat in a hat"
llms -m "Gemini 2.5 Flash Image" --out image "cat in a hat"
```

<Info>üìÅ All generated images are saved to the `~/.llms/cache` folder using their SHA-256 hash as the filename.</Info>

---

## Audio Generation Support

v3 includes built-in support for audio generation with Google's new Text-to-Speech models:

| Model | Description |
|-------|-------------|
| **Gemini 2.5 Flash Preview TTS** | Fast, lightweight TTS |
| **Gemini 2.5 Pro Preview TTS** | High-quality TTS |

<img src="/img/generate-audio.webp" class="my-4 rounded-lg hover:shadow border border-gray-200 dark:border-gray-700"/>

### UI & Command-Line Usage

Available in both the UI and on the command-line using `--out audio`:

```bash
llms --out audio "Merry Christmas"
llms -m gemini-2.5-pro-preview-tts --out audio "Merry Christmas"
```

### Output

Audio files are saved locally and accessible via HTTP URL:

```
Saved files:
/Users/llmspy/.llms/cache/c2/c27b5fd43ebbdbca...acf118.wav
http://localhost:8000/~cache/c2/c27b5fd43ebbdbca...acf118.wav
```

### Playback

**From the command line:**
```bash
play /Users/llmspy/.llms/cache/c2/c27b5fd43ebbdbca...acf118.wav
```

**From the browser:**
Run the server with `llms --serve 8000` and access the URL to play in your browser.

---

## Image Cache & Optimization

A new caching system has been implemented for uploaded images and files. Uploads are now persisted in `~/.llms/cache`, preserving them across messages and sessions.

- **Efficient Storage**: Only cache references are stored in the browser history and sent with chat messages, significantly reducing local storage usage and payload size
- **Persistent Access**: Images remain accessible for previews and downloads even after page reloads
- **Automatic Management**: The system handles file storage and serving transparently, ensuring a smooth user experience

---

## Enhancements & Fixes

### Improved Model Selection
Models can now be selected via CLI or API using flexible resolution logic:
- **Case-Insensitive**: Match models regardless of casing
- **Short Form**: Use short names (e.g., `gpt-4o` instead of `openai/gpt-4o`)
- **Display Names**: Select models by their human-readable **name** (from models.dev)

### Cost Display
Updated cost metrics to consistently display price per **1 Million tokens**.

### Chat History Improvements
Fixed context preservation for Image URLs in chat history, ensuring follow-up requests retain full context.

### Provider Updates

| Provider | Update |
|----------|--------|
| **Anthropic** | Native implementation of the chat method |
| **Ollama** | Fixed endpoint handling (resolved 404 errors) |
| **LMStudio** | New support with dynamic model selection |

---

## v3 Configuration Migration

> ‚ö†Ô∏è **Breaking Change**: The v3 `llms.json` configuration is **not compatible** with v2 as it has been completely overhauled to be a superset of models.dev.

When running llms v3 for the first time, it will:
1. **Automatically backup** your previous `llms.json`
2. **Create a new** `llms.json` with the v3 format

### Manual Cleanup Required

You should also delete your `LlmsThreads` IndexedDB as it's not compatible with v3:

1. Open browser DevTools (F12)
2. Go to **Application** ‚Üí **IndexedDB**
3. Delete the `LlmsThreads` database

---

## Upgrade Instructions

```bash
# Update llms to v3
pip install --upgrade llms

# Update provider definitions
llms --update-providers

# Start the server
llms --serve 8000
```

**Happy holidays from the llms.py team!** üéÑ


---

<ScreenshotsGallery className="mb-8" gridClass="grid grid-cols-1 md:grid-cols-2 gap-4" images={{
    'New Model Selector': '/img/model-selector.webp',
}} />
